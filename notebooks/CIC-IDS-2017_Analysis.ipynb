{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIC-IDS-2017 Dataset Analysis\n",
        "\n",
        "This notebook provides a comprehensive analysis of the CIC-IDS-2017 network intrusion detection dataset.\n",
        "\n",
        "## Dataset Overview\n",
        "The CIC-IDS-2017 dataset is a comprehensive network intrusion detection dataset containing labeled network traffic flows captured over 5 days (Monday to Friday) in July 2017.\n",
        "\n",
        "### Dataset Structure:\n",
        "- **Raw Data** (`GeneratedLabelledFlows/TrafficLabelling/`): Original labeled network flow data\n",
        "- **Processed Data** (`MachineLearningCSV/MachineLearningCVE/`): Same data but processed for machine learning applications\n",
        "\n",
        "### Key Differences:\n",
        "- **Removed**: Flow ID, Source IP, Source Port, Destination IP, Timestamp\n",
        "- **Kept**: All statistical features (73 features) + Label\n",
        "- **Purpose**: Privacy protection while maintaining ML utility\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Structure Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw Data Files:\n",
            "1. Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "2. Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "3. Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "4. Monday-WorkingHours.pcap_ISCX.csv\n",
            "5. Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "6. Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "7. Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "8. Wednesday-workingHours.pcap_ISCX.csv\n",
            "\n",
            "Processed Data Files:\n",
            "1. Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "2. Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "3. Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "4. Monday-WorkingHours.pcap_ISCX.csv\n",
            "5. Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "6. Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "7. Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "8. Wednesday-workingHours.pcap_ISCX.csv\n",
            "\n",
            "Total files in each directory: 8 vs 8\n"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "raw_data_path = \"CIC-IDS-2017/GeneratedLabelledFlows/TrafficLabelling/\"\n",
        "processed_data_path = \"CIC-IDS-2017/MachineLearningCSV/MachineLearningCVE/\"\n",
        "\n",
        "# Get file lists\n",
        "raw_files = [f for f in os.listdir(raw_data_path) if f.endswith('.csv')]\n",
        "processed_files = [f for f in os.listdir(processed_data_path) if f.endswith('.csv')]\n",
        "\n",
        "print(\"Raw Data Files:\")\n",
        "for i, file in enumerate(raw_files, 1):\n",
        "    print(f\"{i}. {file}\")\n",
        "\n",
        "print(\"\\nProcessed Data Files:\")\n",
        "for i, file in enumerate(processed_files, 1):\n",
        "    print(f\"{i}. {file}\")\n",
        "\n",
        "print(f\"\\nTotal files in each directory: {len(raw_files)} vs {len(processed_files)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. File Size and Record Count Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== RAW DATA ANALYSIS ===\n",
            "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv: 225,745 records, 91.65 MB\n",
            "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv: 286,467 records, 97.16 MB\n",
            "Friday-WorkingHours-Morning.pcap_ISCX.csv: 191,033 records, 71.89 MB\n",
            "Monday-WorkingHours.pcap_ISCX.csv: 529,918 records, 256.2 MB\n",
            "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv: 288,602 records, 103.69 MB\n",
            "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv: 458,968 records, 87.77 MB\n",
            "Tuesday-WorkingHours.pcap_ISCX.csv: 445,909 records, 166.6 MB\n",
            "Wednesday-workingHours.pcap_ISCX.csv: 692,703 records, 272.41 MB\n",
            "\n",
            "=== PROCESSED DATA ANALYSIS ===\n",
            "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv: 225,745 records, 73.55 MB\n",
            "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv: 286,467 records, 73.34 MB\n",
            "Friday-WorkingHours-Morning.pcap_ISCX.csv: 191,033 records, 55.62 MB\n",
            "Monday-WorkingHours.pcap_ISCX.csv: 529,918 records, 168.73 MB\n",
            "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv: 288,602 records, 79.25 MB\n",
            "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv: 170,366 records, 49.61 MB\n",
            "Tuesday-WorkingHours.pcap_ISCX.csv: 445,909 records, 128.82 MB\n",
            "Wednesday-workingHours.pcap_ISCX.csv: 692,703 records, 214.74 MB\n",
            "\n",
            "=== COMPARISON SUMMARY ===\n",
            "                                         File  Raw_Records  Processed_Records  Raw_Size_MB  Processed_Size_MB  Record_Diff  Size_Diff_MB\n",
            "           Friday-WorkingHours-Afternoon-DDos       225745             225745        91.65              73.55            0        -18.10\n",
            "       Friday-WorkingHours-Afternoon-PortScan       286467             286467        97.16              73.34            0        -23.82\n",
            "                  Friday-WorkingHours-Morning       191033             191033        71.89              55.62            0        -16.27\n",
            "                          Monday-WorkingHours       529918             529918       256.20             168.73            0        -87.47\n",
            "Thursday-WorkingHours-Afternoon-Infilteration       288602             288602       103.69              79.25            0        -24.44\n",
            "     Thursday-WorkingHours-Morning-WebAttacks       458968             170366        87.77              49.61      -288602        -38.16\n",
            "                         Tuesday-WorkingHours       445909             445909       166.60             128.82            0        -37.78\n",
            "                       Wednesday-workingHours       692703             692703       272.41             214.74            0        -57.67\n"
          ]
        }
      ],
      "source": [
        "def get_file_info(file_path, file_name):\n",
        "    \"\"\"Get file information including record count and size\"\"\"\n",
        "    try:\n",
        "        # Get file size\n",
        "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
        "        \n",
        "        # Get record count\n",
        "        with open(file_path, 'r') as f:\n",
        "            line_count = sum(1 for line in f) - 1  # Subtract header\n",
        "        \n",
        "        return {\n",
        "            'file_name': file_name,\n",
        "            'size_mb': round(file_size, 2),\n",
        "            'records': line_count\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'file_name': file_name,\n",
        "            'size_mb': 0,\n",
        "            'records': 0,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "# Analyze raw data files\n",
        "print(\"=== RAW DATA ANALYSIS ===\")\n",
        "raw_info = []\n",
        "for file in raw_files:\n",
        "    info = get_file_info(os.path.join(raw_data_path, file), file)\n",
        "    raw_info.append(info)\n",
        "    print(f\"{file}: {info['records']:,} records, {info['size_mb']} MB\")\n",
        "\n",
        "print(\"\\n=== PROCESSED DATA ANALYSIS ===\")\n",
        "processed_info = []\n",
        "for file in processed_files:\n",
        "    info = get_file_info(os.path.join(processed_data_path, file), file)\n",
        "    processed_info.append(info)\n",
        "    print(f\"{file}: {info['records']:,} records, {info['size_mb']} MB\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'File': [f.replace('.pcap_ISCX.csv', '') for f in raw_files],\n",
        "    'Raw_Records': [info['records'] for info in raw_info],\n",
        "    'Processed_Records': [info['records'] for info in processed_info],\n",
        "    'Raw_Size_MB': [info['size_mb'] for info in raw_info],\n",
        "    'Processed_Size_MB': [info['size_mb'] for info in processed_info]\n",
        "})\n",
        "\n",
        "comparison_df['Record_Diff'] = comparison_df['Processed_Records'] - comparison_df['Raw_Records']\n",
        "comparison_df['Size_Diff_MB'] = comparison_df['Processed_Size_MB'] - comparison_df['Raw_Size_MB']\n",
        "\n",
        "print(\"\\n=== COMPARISON SUMMARY ===\")\n",
        "print(comparison_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Structure Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== RAW DATA STRUCTURE ===\n",
            "Shape: (5, 85)\n",
            "Columns: 85\n",
            "\n",
            "First 10 columns:\n",
            " 1. 'Flow ID'\n",
            " 2. ' Source IP'\n",
            " 3. ' Source Port'\n",
            " 4. ' Destination IP'\n",
            " 5. ' Destination Port'\n",
            " 6. ' Protocol'\n",
            " 7. ' Timestamp'\n",
            " 8. ' Flow Duration'\n",
            " 9. ' Total Fwd Packets'\n",
            "10. ' Total Backward Packets'\n",
            "\n",
            "=== PROCESSED DATA STRUCTURE ===\n",
            "Shape: (5, 79)\n",
            "Columns: 79\n",
            "\n",
            "First 10 columns:\n",
            " 1. ' Destination Port'\n",
            " 2. ' Flow Duration'\n",
            " 3. ' Total Fwd Packets'\n",
            " 4. ' Total Backward Packets'\n",
            " 5. 'Total Length of Fwd Packets'\n",
            " 6. ' Total Length of Bwd Packets'\n",
            " 7. ' Fwd Packet Length Max'\n",
            " 8. ' Fwd Packet Length Min'\n",
            " 9. ' Fwd Packet Length Mean'\n",
            "10. ' Fwd Packet Length Std'\n",
            "\n",
            "=== REMOVED COLUMNS ===\n",
            "Removed columns: 6\n",
            "- ' Destination IP'\n",
            "- ' Protocol'\n",
            "- ' Source IP'\n",
            "- ' Source Port'\n",
            "- ' Timestamp'\n",
            "- 'Flow ID'\n"
          ]
        }
      ],
      "source": [
        "# Read sample data from both raw and processed datasets\n",
        "raw_sample = pd.read_csv(os.path.join(raw_data_path, raw_files[0]), nrows=5)\n",
        "processed_sample = pd.read_csv(os.path.join(processed_data_path, processed_files[0]), nrows=5)\n",
        "\n",
        "print(\"=== RAW DATA STRUCTURE ===\")\n",
        "print(f\"Shape: {raw_sample.shape}\")\n",
        "print(f\"Columns: {len(raw_sample.columns)}\")\n",
        "print(\"\\nFirst 10 columns:\")\n",
        "for i, col in enumerate(raw_sample.columns[:10], 1):\n",
        "    print(f\"{i:2d}. {repr(col)}\")\n",
        "\n",
        "print(\"\\n=== PROCESSED DATA STRUCTURE ===\")\n",
        "print(f\"Shape: {processed_sample.shape}\")\n",
        "print(f\"Columns: {len(processed_sample.columns)}\")\n",
        "print(\"\\nFirst 10 columns:\")\n",
        "for i, col in enumerate(processed_sample.columns[:10], 1):\n",
        "    print(f\"{i:2d}. {repr(col)}\")\n",
        "\n",
        "print(\"\\n=== REMOVED COLUMNS ===\")\n",
        "raw_cols = set(raw_sample.columns)\n",
        "processed_cols = set(processed_sample.columns)\n",
        "removed_cols = raw_cols - processed_cols\n",
        "print(f\"Removed columns: {len(removed_cols)}\")\n",
        "for col in sorted(removed_cols):\n",
        "    print(f\"- {repr(col)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Attack Type Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ATTACK TYPE DISTRIBUTION ===\n",
            "\n",
            "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv:\n",
            "Total records: 225,745\n",
            "  DDoS: 128,027 (56.71%)\n",
            "  BENIGN: 97,718 (43.29%)\n",
            "\n",
            "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv:\n",
            "Total records: 286,467\n",
            "  PortScan: 158,930 (55.48%)\n",
            "  BENIGN: 127,537 (44.52%)\n",
            "\n",
            "Friday-WorkingHours-Morning.pcap_ISCX.csv:\n",
            "Total records: 191,033\n",
            "  BENIGN: 189,067 (98.97%)\n",
            "  Bot: 1,966 (1.03%)\n",
            "\n",
            "Monday-WorkingHours.pcap_ISCX.csv:\n",
            "Total records: 529,918\n",
            "  BENIGN: 529,918 (100.00%)\n",
            "\n",
            "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv:\n",
            "Total records: 288,602\n",
            "  BENIGN: 288,566 (99.99%)\n",
            "  Infiltration: 36 (0.01%)\n",
            "\n",
            "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv:\n",
            "Total records: 170,366\n",
            "  BENIGN: 168,186 (98.72%)\n",
            "  Web Attack � Brute Force: 1,507 (0.88%)\n",
            "  Web Attack � XSS: 652 (0.38%)\n",
            "  Web Attack � Sql Injection: 21 (0.01%)\n",
            "\n",
            "Tuesday-WorkingHours.pcap_ISCX.csv:\n",
            "Total records: 445,909\n",
            "  BENIGN: 432,074 (96.90%)\n",
            "  FTP-Patator: 7,938 (1.78%)\n",
            "  SSH-Patator: 5,897 (1.32%)\n",
            "\n",
            "Wednesday-workingHours.pcap_ISCX.csv:\n",
            "Total records: 692,703\n",
            "  BENIGN: 440,031 (63.52%)\n",
            "  DoS Hulk: 231,073 (33.36%)\n",
            "  DoS GoldenEye: 10,293 (1.49%)\n",
            "  DoS slowloris: 5,796 (0.84%)\n",
            "  DoS Slowhttptest: 5,499 (0.79%)\n",
            "  Heartbleed: 11 (0.00%)\n",
            "\n",
            "=== TOTAL DISTRIBUTION ACROSS ALL FILES ===\n",
            "BENIGN: 2,273,097 (80.30%)\n",
            "Bot: 1,966 (0.07%)\n",
            "DDoS: 128,027 (4.52%)\n",
            "DoS GoldenEye: 10,293 (0.36%)\n",
            "DoS Hulk: 231,073 (8.16%)\n",
            "DoS Slowhttptest: 5,499 (0.19%)\n",
            "DoS slowloris: 5,796 (0.20%)\n",
            "FTP-Patator: 7,938 (0.28%)\n",
            "Heartbleed: 11 (0.00%)\n",
            "Infiltration: 36 (0.00%)\n",
            "PortScan: 158,930 (5.61%)\n",
            "SSH-Patator: 5,897 (0.21%)\n",
            "Web Attack � Brute Force: 1,507 (0.05%)\n",
            "Web Attack � Sql Injection: 21 (0.00%)\n",
            "Web Attack � XSS: 652 (0.02%)\n",
            "\n",
            "Total records: 2,830,743\n"
          ]
        }
      ],
      "source": [
        "def analyze_attack_distribution(file_path, file_name):\n",
        "    \"\"\"Analyze attack type distribution in a file\"\"\"\n",
        "    try:\n",
        "        # Read the file\n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # Get label column (last column)\n",
        "        label_col = df.columns[-1]\n",
        "        \n",
        "        # Count attack types\n",
        "        attack_counts = df[label_col].value_counts()\n",
        "        \n",
        "        return {\n",
        "            'file_name': file_name,\n",
        "            'total_records': len(df),\n",
        "            'attack_distribution': attack_counts.to_dict()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'file_name': file_name,\n",
        "            'total_records': 0,\n",
        "            'attack_distribution': {},\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "# Analyze attack distribution in processed data\n",
        "print(\"=== ATTACK TYPE DISTRIBUTION ===\")\n",
        "attack_analysis = []\n",
        "\n",
        "for file in processed_files:\n",
        "    file_path = os.path.join(processed_data_path, file)\n",
        "    analysis = analyze_attack_distribution(file_path, file)\n",
        "    attack_analysis.append(analysis)\n",
        "    \n",
        "    print(f\"\\n{file}:\")\n",
        "    print(f\"Total records: {analysis['total_records']:,}\")\n",
        "    if 'attack_distribution' in analysis:\n",
        "        for attack_type, count in analysis['attack_distribution'].items():\n",
        "            percentage = (count / analysis['total_records']) * 100\n",
        "            print(f\"  {attack_type}: {count:,} ({percentage:.2f}%)\")\n",
        "\n",
        "# Calculate total distribution\n",
        "print(\"\\n=== TOTAL DISTRIBUTION ACROSS ALL FILES ===\")\n",
        "total_distribution = {}\n",
        "total_records = 0\n",
        "\n",
        "for analysis in attack_analysis:\n",
        "    total_records += analysis['total_records']\n",
        "    for attack_type, count in analysis['attack_distribution'].items():\n",
        "        total_distribution[attack_type] = total_distribution.get(attack_type, 0) + count\n",
        "\n",
        "for attack_type, count in sorted(total_distribution.items()):\n",
        "    percentage = (count / total_records) * 100\n",
        "    print(f\"{attack_type}: {count:,} ({percentage:.2f}%)\")\n",
        "\n",
        "print(f\"\\nTotal records: {total_records:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Quality Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA QUALITY ANALYSIS ===\n",
            "\n",
            "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv:\n",
            "  Sample size: 1000\n",
            "  Missing values: 0\n",
            "  -1 values: 996\n",
            "  Infinite values: 2\n",
            "  Constant columns: 13\n",
            "  Duplicate rows: 0\n",
            "\n",
            "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv:\n",
            "  Sample size: 1000\n",
            "  Missing values: 0\n",
            "  -1 values: 1277\n",
            "  Infinite values: 2\n",
            "  Constant columns: 13\n",
            "  Duplicate rows: 11\n",
            "\n",
            "Friday-WorkingHours-Morning.pcap_ISCX.csv:\n",
            "  Sample size: 1000\n",
            "  Missing values: 0\n",
            "  -1 values: 1146\n",
            "  Infinite values: 2\n",
            "  Constant columns: 13\n",
            "  Duplicate rows: 11\n",
            "\n",
            "Monday-WorkingHours.pcap_ISCX.csv:\n",
            "  Sample size: 1000\n",
            "  Missing values: 0\n",
            "  -1 values: 771\n",
            "  Infinite values: 20\n",
            "  Constant columns: 11\n",
            "  Duplicate rows: 51\n",
            "\n",
            "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv:\n",
            "  Sample size: 1000\n",
            "  Missing values: 0\n",
            "  -1 values: 1132\n",
            "  Infinite values: 0\n",
            "  Constant columns: 13\n",
            "  Duplicate rows: 2\n",
            "\n",
            "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv:\n",
            "  Sample size: 1000\n",
            "  Missing values: 0\n",
            "  -1 values: 1282\n",
            "  Infinite values: 8\n",
            "  Constant columns: 13\n",
            "  Duplicate rows: 14\n",
            "\n",
            "Tuesday-WorkingHours.pcap_ISCX.csv:\n",
            "  Sample size: 1000\n",
            "  Missing values: 4\n",
            "  -1 values: 509\n",
            "  Infinite values: 8\n",
            "  Constant columns: 13\n",
            "  Duplicate rows: 4\n",
            "  Missing by column:\n",
            "    Flow Bytes/s: 4\n",
            "\n",
            "Wednesday-workingHours.pcap_ISCX.csv:\n",
            "  Sample size: 1000\n",
            "  Missing values: 0\n",
            "  -1 values: 546\n",
            "  Infinite values: 0\n",
            "  Constant columns: 11\n",
            "  Duplicate rows: 4\n",
            "\n",
            "=== SUMMARY STATISTICS ===\n",
            "Total missing values: 4\n",
            "Total -1 values: 7,659\n",
            "Total infinite values: 42\n",
            "Total constant columns: 100\n",
            "Total duplicate rows: 97\n"
          ]
        }
      ],
      "source": [
        "def analyze_data_quality(file_path, file_name, sample_size=1000):\n",
        "    \"\"\"Analyze data quality issues in a file\"\"\"\n",
        "    try:\n",
        "        # Read sample data\n",
        "        df = pd.read_csv(file_path, nrows=sample_size)\n",
        "        \n",
        "        # Check missing values\n",
        "        missing_values = df.isnull().sum()\n",
        "        total_missing = missing_values.sum()\n",
        "        \n",
        "        # Check -1 values (common placeholder)\n",
        "        minus_one_count = (df == -1).sum().sum()\n",
        "        \n",
        "        # Check infinite values\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        inf_count = 0\n",
        "        for col in numeric_cols:\n",
        "            inf_count += np.isinf(df[col]).sum()\n",
        "        \n",
        "        # Check for constant columns\n",
        "        constant_cols = []\n",
        "        for col in df.columns:\n",
        "            if df[col].nunique() == 1:\n",
        "                constant_cols.append(col)\n",
        "        \n",
        "        # Check for duplicate rows\n",
        "        duplicates = df.duplicated().sum()\n",
        "        \n",
        "        return {\n",
        "            'file_name': file_name,\n",
        "            'sample_size': len(df),\n",
        "            'total_missing': total_missing,\n",
        "            'minus_one_count': minus_one_count,\n",
        "            'inf_count': inf_count,\n",
        "            'constant_cols': len(constant_cols),\n",
        "            'duplicates': duplicates,\n",
        "            'missing_by_column': missing_values[missing_values > 0].to_dict()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'file_name': file_name,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "# Analyze data quality for all files\n",
        "print(\"=== DATA QUALITY ANALYSIS ===\")\n",
        "quality_analysis = []\n",
        "\n",
        "for file in processed_files:\n",
        "    file_path = os.path.join(processed_data_path, file)\n",
        "    analysis = analyze_data_quality(file_path, file)\n",
        "    quality_analysis.append(analysis)\n",
        "    \n",
        "    print(f\"\\n{file}:\")\n",
        "    if 'error' in analysis:\n",
        "        print(f\"  Error: {analysis['error']}\")\n",
        "    else:\n",
        "        print(f\"  Sample size: {analysis['sample_size']}\")\n",
        "        print(f\"  Missing values: {analysis['total_missing']}\")\n",
        "        print(f\"  -1 values: {analysis['minus_one_count']}\")\n",
        "        print(f\"  Infinite values: {analysis['inf_count']}\")\n",
        "        print(f\"  Constant columns: {analysis['constant_cols']}\")\n",
        "        print(f\"  Duplicate rows: {analysis['duplicates']}\")\n",
        "        \n",
        "        if analysis['missing_by_column']:\n",
        "            print(f\"  Missing by column:\")\n",
        "            for col, count in analysis['missing_by_column'].items():\n",
        "                print(f\"    {col}: {count}\")\n",
        "\n",
        "# Calculate totals\n",
        "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
        "total_missing = sum(analysis.get('total_missing', 0) for analysis in quality_analysis)\n",
        "total_minus_one = sum(analysis.get('minus_one_count', 0) for analysis in quality_analysis)\n",
        "total_inf = sum(analysis.get('inf_count', 0) for analysis in quality_analysis)\n",
        "total_constant = sum(analysis.get('constant_cols', 0) for analysis in quality_analysis)\n",
        "total_duplicates = sum(analysis.get('duplicates', 0) for analysis in quality_analysis)\n",
        "\n",
        "print(f\"Total missing values: {total_missing:,}\")\n",
        "print(f\"Total -1 values: {total_minus_one:,}\")\n",
        "print(f\"Total infinite values: {total_inf:,}\")\n",
        "print(f\"Total constant columns: {total_constant}\")\n",
        "print(f\"Total duplicate rows: {total_duplicates:,}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
